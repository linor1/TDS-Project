{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split# Load the dataset into a pandas DataFrame\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature values:\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "\n",
      "Processed feature values:\n",
      "[[5. 0. 5. ... 9. 5. 4.]\n",
      " [6. 2. 6. ... 6. 2. 2.]\n",
      " [6. 3. 5. ... 8. 4. 2.]\n",
      " ...\n",
      " [4. 6. 4. ... 4. 1. 1.]\n",
      " [6. 6. 6. ... 9. 4. 4.]\n",
      " [0. 5. 0. ... 0. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline for preprocessing the features\n",
    "preprocessor = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform'))\n",
    "])\n",
    "\n",
    "# Preprocess the features using the pipeline\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Print the original and processed feature values for the first five rows\n",
    "print(\"Original feature values:\")\n",
    "print(X)\n",
    "print(\"\\nProcessed feature values:\")\n",
    "print(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "## Breast cancer dataset - split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a decision tree on the training set\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy score on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Accuracy:\", round(acc,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset - chi squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split# Load the dataset into a pandas DataFrame\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer_data = load_breast_cancer()\n",
    "print(\"feature names: \", breast_cancer_data.feature_names)\n",
    "df = pd.DataFrame(data=breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
    "df['target'] = breast_cancer_data.target\n",
    "\n",
    "# Create a Pandas dataframe from the data\n",
    "#df = pd.DataFrame(data=breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
    "#X, y = breast_cancer_data.data, breast_cancer_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int32"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "chi_scores = chi2(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.66104917e+02, 9.38975081e+01, 2.01110286e+03, 5.39916559e+04,\n",
       "        1.49899264e-01, 5.40307549e+00, 1.97123536e+01, 1.05440354e+01,\n",
       "        2.57379775e-01, 7.43065536e-05, 3.46752472e+01, 9.79353970e-03,\n",
       "        2.50571896e+02, 8.75850471e+03, 3.26620664e-03, 6.13785332e-01,\n",
       "        1.04471761e+00, 3.05231563e-01, 8.03633831e-05, 6.37136566e-03,\n",
       "        4.91689157e+02, 1.74449400e+02, 3.66503542e+03, 1.12598432e+05,\n",
       "        3.97365694e-01, 1.93149220e+01, 3.95169151e+01, 1.34854195e+01,\n",
       "        1.29886140e+00, 2.31522407e-01]),\n",
       " array([8.01397628e-060, 3.32292194e-022, 0.00000000e+000, 0.00000000e+000,\n",
       "        6.98631644e-001, 2.01012999e-002, 9.00175712e-006, 1.16563638e-003,\n",
       "        6.11926026e-001, 9.93122221e-001, 3.89553429e-009, 9.21168192e-001,\n",
       "        1.94877489e-056, 0.00000000e+000, 9.54425121e-001, 4.33366115e-001,\n",
       "        3.06726812e-001, 5.80621137e-001, 9.92847410e-001, 9.36379753e-001,\n",
       "        6.11324751e-109, 7.89668299e-040, 0.00000000e+000, 0.00000000e+000,\n",
       "        5.28452867e-001, 1.10836762e-005, 3.25230064e-010, 2.40424384e-004,\n",
       "        2.54421307e-001, 6.30397277e-001]))"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_values.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean fractal dimension      9.931222e-01\n",
      "symmetry error              9.928474e-01\n",
      "smoothness error            9.544251e-01\n",
      "fractal dimension error     9.363798e-01\n",
      "texture error               9.211682e-01\n",
      "mean smoothness             6.986316e-01\n",
      "worst fractal dimension     6.303973e-01\n",
      "mean symmetry               6.119260e-01\n",
      "concave points error        5.806211e-01\n",
      "worst smoothness            5.284529e-01\n",
      "compactness error           4.333661e-01\n",
      "concavity error             3.067268e-01\n",
      "worst symmetry              2.544213e-01\n",
      "mean compactness            2.010130e-02\n",
      "mean concave points         1.165636e-03\n",
      "worst concave points        2.404244e-04\n",
      "worst compactness           1.108368e-05\n",
      "mean concavity              9.001757e-06\n",
      "radius error                3.895534e-09\n",
      "worst concavity             3.252301e-10\n",
      "mean texture                3.322922e-22\n",
      "worst texture               7.896683e-40\n",
      "perimeter error             1.948775e-56\n",
      "mean radius                 8.013976e-60\n",
      "worst radius               6.113248e-109\n",
      "mean perimeter              0.000000e+00\n",
      "worst area                  0.000000e+00\n",
      "worst perimeter             0.000000e+00\n",
      "mean area                   0.000000e+00\n",
      "area error                  0.000000e+00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the higher p-value features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['mean fractal dimension' 'symmetry error' 'smoothness error'\\n 'fractal dimension error' 'texture error'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-602-d94c79794583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean fractal dimension'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'symmetry error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'smoothness error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fractal dimension error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'texture error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4307\u001b[0m         \"\"\"\n\u001b[1;32m-> 4308\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['mean fractal dimension' 'symmetry error' 'smoothness error'\\n 'fractal dimension error' 'texture error'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = df.drop(['mean fractal dimension','symmetry error','smoothness error','fractal dimension error','texture error'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (largest diff): 81.6%\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier on the training set\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compute accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Accuracy (largest diff): {round(accuracy_smallest,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dropped five features, and with k-means clustering, we intend to drop the same number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset - K-means clustering part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_features:  30\n",
      "\n",
      "K=5\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 5\n",
      "Accuracy (largest diff): 94.7%\n",
      "Number of features in the selected dataset (smallest diff): 5\n",
      "Accuracy (largest diff): 71.9%\n",
      "\n",
      "K=6\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 6\n",
      "Accuracy (largest diff): 94.7%\n",
      "Number of features in the selected dataset (smallest diff): 6\n",
      "Accuracy (largest diff): 83.3%\n",
      "\n",
      "K=7\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 7\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 7\n",
      "Accuracy (largest diff): 85.1%\n",
      "\n",
      "K=8\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 8\n",
      "Accuracy (largest diff): 91.2%\n",
      "Number of features in the selected dataset (smallest diff): 8\n",
      "Accuracy (largest diff): 77.2%\n",
      "\n",
      "K=9\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 9\n",
      "Accuracy (largest diff): 93.0%\n",
      "Number of features in the selected dataset (smallest diff): 9\n",
      "Accuracy (largest diff): 79.8%\n",
      "\n",
      "K=10\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 10\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 10\n",
      "Accuracy (largest diff): 78.9%\n",
      "\n",
      "K=11\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 11\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 11\n",
      "Accuracy (largest diff): 81.6%\n",
      "\n",
      "K=12\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 12\n",
      "Accuracy (largest diff): 93.0%\n",
      "Number of features in the selected dataset (smallest diff): 12\n",
      "Accuracy (largest diff): 84.2%\n",
      "\n",
      "K=13\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 13\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 13\n",
      "Accuracy (largest diff): 78.9%\n",
      "\n",
      "K=14\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 14\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 14\n",
      "Accuracy (largest diff): 84.2%\n",
      "\n",
      "K=15\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 15\n",
      "Accuracy (largest diff): 91.2%\n",
      "Number of features in the selected dataset (smallest diff): 15\n",
      "Accuracy (largest diff): 86.0%\n",
      "\n",
      "K=16\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 16\n",
      "Accuracy (largest diff): 91.2%\n",
      "Number of features in the selected dataset (smallest diff): 16\n",
      "Accuracy (largest diff): 86.0%\n",
      "\n",
      "K=17\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 17\n",
      "Accuracy (largest diff): 92.1%\n",
      "Number of features in the selected dataset (smallest diff): 17\n",
      "Accuracy (largest diff): 85.1%\n",
      "\n",
      "K=18\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 18\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 18\n",
      "Accuracy (largest diff): 86.8%\n",
      "\n",
      "K=19\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 19\n",
      "Accuracy (largest diff): 93.0%\n",
      "Number of features in the selected dataset (smallest diff): 19\n",
      "Accuracy (largest diff): 84.2%\n",
      "\n",
      "K=20\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 20\n",
      "Accuracy (largest diff): 94.7%\n",
      "Number of features in the selected dataset (smallest diff): 20\n",
      "Accuracy (largest diff): 86.0%\n",
      "\n",
      "K=21\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 21\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 21\n",
      "Accuracy (largest diff): 87.7%\n",
      "\n",
      "K=22\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 22\n",
      "Accuracy (largest diff): 92.1%\n",
      "Number of features in the selected dataset (smallest diff): 22\n",
      "Accuracy (largest diff): 91.2%\n",
      "\n",
      "K=23\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 23\n",
      "Accuracy (largest diff): 92.1%\n",
      "Number of features in the selected dataset (smallest diff): 23\n",
      "Accuracy (largest diff): 87.7%\n",
      "\n",
      "K=24\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 24\n",
      "Accuracy (largest diff): 91.2%\n",
      "Number of features in the selected dataset (smallest diff): 24\n",
      "Accuracy (largest diff): 93.0%\n",
      "\n",
      "K=25\n",
      "Number of features in the dataset: 30\n",
      "Number of features in the selected dataset (largest diff): 25\n",
      "Accuracy (largest diff): 93.9%\n",
      "Number of features in the selected dataset (smallest diff): 25\n",
      "Accuracy (largest diff): 93.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset from scikit-learn\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "data = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "# Split the input features and target variable\n",
    "X = data.iloc[:, :]\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns.tolist()\n",
    "number_of_features = X.shape[1]\n",
    "print(\"number_of_features: \",number_of_features)\n",
    "for k in range(5, 26):\n",
    "    print(f\"\\nK={k}\")\n",
    "    \n",
    "    # Perform K-means clustering with K clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Compute the mean value of each feature for each cluster\n",
    "    cluster_means = [np.mean(X[kmeans.labels_ == i], axis=0) for i in range(kmeans.n_clusters)]\n",
    "\n",
    "    # Compute the absolute difference in means between clusters for each feature\n",
    "    diff_means = np.abs(np.diff(cluster_means, axis=0))\n",
    "\n",
    "    # Select the features with the largest difference in means between clusters\n",
    "    selected_features = np.argsort(np.sum(diff_means, axis=0))[::-1][:k]\n",
    "    \n",
    "    # Select the features with the smallest difference in means between clusters\n",
    "    selected_features_smallest = np.argsort(np.sum(diff_means, axis=0))[:k]\n",
    "\n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    print(f\"Number of features in the dataset: {number_of_features}\")\n",
    "    print(f\"Number of features in the selected dataset (largest diff): {len(selected_features)}\")\n",
    "    print(f\"Accuracy (largest diff): {round(accuracy,1)}%\")\n",
    "    \n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected_smallest = X.iloc[:, selected_features_smallest]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_smallest, X_test_smallest, y_train_smallest, y_test_smallest = train_test_split(X_selected_smallest, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf_smallest = DecisionTreeClassifier(random_state=42)\n",
    "    clf_smallest.fit(X_train_smallest, y_train_smallest)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred_smallest = clf_smallest.predict(X_test_smallest)\n",
    "    accuracy_smallest = accuracy_score(y_test_smallest, y_pred_smallest)*100\n",
    "\n",
    "    print(f\"Number of features in the selected dataset (smallest diff): {len(selected_features_smallest)}\")\n",
    "    print(f\"Accuracy (largest diff): {round(accuracy_smallest,1)}%\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 25 selected features the accuracy is 93.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset - K-means clustering part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns:  30\n",
      "k =  5 :\n",
      "Accuracy for top 5 features with highest variance: 93.0\n",
      "Accuracy for top 5 features with lowest variance: 73.7\n",
      "k =  6 :\n",
      "Accuracy for top 6 features with highest variance: 93.0\n",
      "Accuracy for top 6 features with lowest variance: 77.2\n",
      "k =  7 :\n",
      "Accuracy for top 7 features with highest variance: 93.0\n",
      "Accuracy for top 7 features with lowest variance: 81.6\n",
      "k =  8 :\n",
      "Accuracy for top 8 features with highest variance: 93.9\n",
      "Accuracy for top 8 features with lowest variance: 83.3\n",
      "k =  9 :\n",
      "Accuracy for top 9 features with highest variance: 93.0\n",
      "Accuracy for top 9 features with lowest variance: 77.2\n",
      "k =  10 :\n",
      "Accuracy for top 10 features with highest variance: 93.9\n",
      "Accuracy for top 10 features with lowest variance: 86.0\n",
      "k =  11 :\n",
      "Accuracy for top 11 features with highest variance: 93.0\n",
      "Accuracy for top 11 features with lowest variance: 87.7\n",
      "k =  12 :\n",
      "Accuracy for top 12 features with highest variance: 93.0\n",
      "Accuracy for top 12 features with lowest variance: 85.1\n",
      "k =  13 :\n",
      "Accuracy for top 13 features with highest variance: 93.9\n",
      "Accuracy for top 13 features with lowest variance: 85.1\n",
      "k =  14 :\n",
      "Accuracy for top 14 features with highest variance: 93.0\n",
      "Accuracy for top 14 features with lowest variance: 84.2\n",
      "k =  15 :\n",
      "Accuracy for top 15 features with highest variance: 91.2\n",
      "Accuracy for top 15 features with lowest variance: 85.1\n",
      "k =  16 :\n",
      "Accuracy for top 16 features with highest variance: 92.1\n",
      "Accuracy for top 16 features with lowest variance: 86.8\n",
      "k =  17 :\n",
      "Accuracy for top 17 features with highest variance: 90.4\n",
      "Accuracy for top 17 features with lowest variance: 85.1\n",
      "k =  18 :\n",
      "Accuracy for top 18 features with highest variance: 91.2\n",
      "Accuracy for top 18 features with lowest variance: 85.1\n",
      "k =  19 :\n",
      "Accuracy for top 19 features with highest variance: 93.0\n",
      "Accuracy for top 19 features with lowest variance: 86.8\n",
      "k =  20 :\n",
      "Accuracy for top 20 features with highest variance: 93.0\n",
      "Accuracy for top 20 features with lowest variance: 85.1\n",
      "k =  21 :\n",
      "Accuracy for top 21 features with highest variance: 92.1\n",
      "Accuracy for top 21 features with lowest variance: 86.8\n",
      "k =  22 :\n",
      "Accuracy for top 22 features with highest variance: 91.2\n",
      "Accuracy for top 22 features with lowest variance: 88.6\n",
      "k =  23 :\n",
      "Accuracy for top 23 features with highest variance: 92.1\n",
      "Accuracy for top 23 features with lowest variance: 92.1\n",
      "k =  24 :\n",
      "Accuracy for top 24 features with highest variance: 93.0\n",
      "Accuracy for top 24 features with lowest variance: 93.9\n",
      "k =  25 :\n",
      "Accuracy for top 25 features with highest variance: 93.0\n",
      "Accuracy for top 25 features with lowest variance: 93.0\n",
      "k =  26 :\n",
      "Accuracy for top 26 features with highest variance: 91.2\n",
      "Accuracy for top 26 features with lowest variance: 93.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column\n",
    "print(\"number of columns: \", X.shape[1])\n",
    "\n",
    "# Fit K-means clustering to the data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Compute the variance of each feature within each cluster\n",
    "variances = np.zeros((3, X.shape[1]))\n",
    "for i in range(3):\n",
    "    cluster_data = X[labels == i]\n",
    "    variances[i] = np.var(cluster_data, axis=0)\n",
    "\n",
    "# Select the features with the highest variance within the cluster with the most data points\n",
    "most_common_cluster = np.argmax(np.bincount(labels))\n",
    "for k in range(5, 27):\n",
    "    selected_features = np.argsort(variances[most_common_cluster])[::-1][:k] # select top k features with highest variance\n",
    "    # select the features with the lowest variance within the most common cluster\n",
    "    low_selected_features = np.argsort(variances[most_common_cluster])[:k]  # select top k features with lowest variance\n",
    "    # Use the selected features to transform the data\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    low_X_selected = X.iloc[:, low_selected_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"k = \",k,\":\")\n",
    "    print(f\"Accuracy for top {k} features with highest variance:\", round(accuracy,1))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(low_X_selected, y, test_size=0.2, random_state=42)\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(f\"Accuracy for top {k} features with lowest variance:\", round(accuracy,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we receved: Accuracy for top 25 features with highest variance: 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset - load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "# Print the feature names and target names\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset - split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset - chi squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                   header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "\n",
    "# Define the features and target variable\n",
    "X = iris.iloc[:, :-1] # Features\n",
    "Y = iris.iloc[:, -1]  # Target variable\n",
    "\n",
    "# Define a function to compute the chi-squared statistic and p-value\n",
    "def chi_squared_test(X, Y):\n",
    "    statistic = []\n",
    "    p_value = []\n",
    "    for col in X.columns:  # loop over all features\n",
    "        observed = pd.crosstab(X[col], Y)\n",
    "        stat, pval, dof, expected = chi2_contingency(observed)\n",
    "        statistic.append(stat)\n",
    "        p_value.append(pval)\n",
    "    return pd.DataFrame({'Feature': X.columns, 'Chi-Squared Statistic': statistic, 'p-value': p_value})\n",
    "\n",
    "# Call the function to compute the chi-squared statistic and p-value for each feature\n",
    "results = chi_squared_test(X, Y)\n",
    "\n",
    "# Print the results\n",
    "print(results)\n",
    "\n",
    "# Set the p-value threshold for significance\n",
    "p_value_threshold = 0.1\n",
    "\n",
    "# Print the significant features\n",
    "print(results['p-value'])\n",
    "significant_features = results[results['p-value'] < p_value_threshold]['Feature']\n",
    "print(\"******************************\")\n",
    "print(significant_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset - chi squared test part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "dtf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "dtf['target'] = iris.target\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_columns = ['target']\n",
    "\n",
    "# Perform chi-square test over all columns\n",
    "chi_res = []\n",
    "for i1, c1 in enumerate(categorical_columns):\n",
    "    for i2, c2 in enumerate(dtf.columns[:-1]):\n",
    "        try:\n",
    "            contingency_table = pd.crosstab(dtf[c1], dtf[c2])\n",
    "            c, p, _, _ = stats.chi2_contingency(contingency_table)\n",
    "        except:\n",
    "            c = None\n",
    "            p = None\n",
    "        chi_res.append({'col1': c1, 'col2': c2, 'score': c, 'p_val': p})\n",
    "\n",
    "# Print the results\n",
    "for res in chi_res:\n",
    "    print(res)\n",
    "chi_df= pd.DataFrame(chi_res)\n",
    "chi_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_df[chi_df.p_val>0.3].sort_values('p_val',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a decision tree on the training set\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy score on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "print(\"feature names: \", diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "print(\"data: \", diabetes.data)\n",
    "print(\"target: \", diabetes.target)\n",
    "# Create a pandas dataframe with feature data\n",
    "df_features = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# Create a pandas dataframe with target data and rename the column\n",
    "df_target = pd.DataFrame(data=diabetes.target, columns=['target'])\n",
    "\n",
    "# Concatenate the feature and target dataframes horizontally\n",
    "df = pd.concat([df_features, df_target], axis=1)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df.head())\n",
    "print(\"feature names: \", diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "print(\"data: \", diabetes.data)\n",
    "print(\"target: \", diabetes.target)\n",
    "# Create a pandas dataframe with feature data\n",
    "df_features = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# Create a pandas dataframe with target data and rename the column\n",
    "df_target = pd.DataFrame(data=diabetes.target, columns=['target'])\n",
    "\n",
    "# Concatenate the feature and target dataframes horizontally\n",
    "df = pd.concat([df_features, df_target], axis=1)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df.head())\n",
    "print(\"feature names: \", diabetes.feature_names)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diabetes dataset and split it into training and test sets\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target,test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize the decision tree regressor and fit it to the training data\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the decision tree on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root mean squared error on test set: {rmse}\")\n",
    "\n",
    "# Evaluate the model on the test data and calculate the accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data by scaling the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a decision tree regression model on the training data\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data and calculate the accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred.round())\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[4 7 5 8 9 2 6 3 0 1]\n",
      "number of features in the dataset:  10\n",
      "number of features in the selected dataset:  10\n",
      "Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "number_of_features = X.shape[1]\n",
    "print(number_of_features)\n",
    "\n",
    "# Perform K-means clustering with K=3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Compute the mean value of each feature for each cluster\n",
    "cluster_means = [np.mean(X[kmeans.labels_ == i, :], axis=0) for i in range(kmeans.n_clusters)]\n",
    "\n",
    "# Compute the absolute difference in means between clusters for each feature\n",
    "diff_means = np.abs(np.diff(cluster_means, axis=0))\n",
    "\n",
    "# Select the features with the largest difference in means between clusters\n",
    "selected_features = np.argsort(np.sum(diff_means, axis=0))[::-1][:10]\n",
    "\n",
    "# Filter the input data to keep only the selected features\n",
    "X_selected = X[:, selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier on the training set\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compute accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "# Print the selected features\n",
    "print(selected_features)\n",
    "selected_feature_names = [feature_names[i] for i in selected_features]\n",
    "print(\"number of features in the dataset: \",number_of_features)\n",
    "print(\"number of features in the selected dataset: \",len(selected_feature_names))\n",
    "print(f\"Accuracy: {round(accuracy, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data - wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "wine = load_wine()\n",
    "print(\"feature names: \", wine.feature_names)\n",
    "# Create a Pandas dataframe from the data\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = wine.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int32"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree - wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "# Create Decision Tree classifer object\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "model = model.fit(x_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"weighted\" parameter is used to calculate the precision \n",
    "for each class and then average them based on their support (the number of samples in each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Calculate accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate mean squared error using sklearn's mean_squared_error function\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.6909283518073533\n",
      "Recall: 0.6883116883116883\n",
      "F1 score: 0.6895043731778426\n",
      "Mean Squared Error: 0.3116883116883117\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",round(accuracy,1))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chisquared test - wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "chi_scores = chi2(X,y)\n",
    "chi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ash has higher the p-value,\n",
    "it says that this variables is independent of the repsone and can not be considered for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71               15.6      127.0           2.80   \n",
       "1    13.20        1.78               11.2      100.0           2.65   \n",
       "2    13.16        2.36               18.6      101.0           2.80   \n",
       "3    14.37        1.95               16.8      113.0           3.85   \n",
       "4    13.24        2.59               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop_1 = df.drop('ash', axis=1)\n",
    "df_drop_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_drop_1.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df_drop_1.iloc[:, -1]   # Select the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "# Create Decision Tree classifer object\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "model = model.fit(x_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation using Accuracy score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Calculate accuracy using sklearn's recall_score function\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate recall using sklearn's recall_score function\n",
    "recall =metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate F1 score using sklearn's f1_score function\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate mean squared error using sklearn's mean_squared_error function\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n",
      "precision: 0.974074074074074\n",
      "Recall:  0.9722222222222222\n",
      "F1 score: 0.9717752234993614\n",
      "Mean Squared Error: 0.027777777777777776\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"precision:\",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71               15.6      127.0           2.80   \n",
       "1    13.20        1.78               11.2      100.0           2.65   \n",
       "2    13.16        2.36               18.6      101.0           2.80   \n",
       "3    14.37        1.95               16.8      113.0           3.85   \n",
       "4    13.24        2.59               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06             2.29             5.64  1.04   \n",
       "1        2.76             1.28             4.38  1.05   \n",
       "2        3.24             2.81             5.68  1.03   \n",
       "3        3.49             2.18             7.80  0.86   \n",
       "4        2.69             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop_2 = df_drop_1.drop('nonflavanoid_phenols', axis=1)\n",
    "df_drop_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_drop_2.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df_drop_2.iloc[:, -1]   # Select the last column\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "# Create Decision Tree classifer object\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "model = model.fit(x_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation using Accuracy score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Calculate accuracy using sklearn's recall_score function\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate recall using sklearn's recall_score function\n",
    "recall =metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate F1 score using sklearn's f1_score function\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate mean squared error using sklearn's mean_squared_error function\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n",
      "precision: 0.974074074074074\n",
      "Recall:  0.9722222222222222\n",
      "F1 score: 0.9717752234993614\n",
      "Mean Squared Error: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"precision:\",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means featrue selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs K-means clustering with different values of K and then selects the features with the largest and smallest differences in means between the clusters. It then trains a decision tree classifier on the selected features and computes the accuracy of the classifier on a test set.\n",
    "\n",
    "By comparing the accuracies obtained using the features with the largest and smallest differences in means between the clusters, this code helps to test the effectiveness of the K-means feature selection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_features:  13\n",
      "\n",
      "K=5\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 5\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 5\n",
      "Accuracy (smallest diff): 77.8%\n",
      "\n",
      "K=6\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 6\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 6\n",
      "Accuracy (smallest diff): 75.0%\n",
      "\n",
      "K=7\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 7\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 7\n",
      "Accuracy (smallest diff): 77.8%\n",
      "\n",
      "K=8\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 8\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 8\n",
      "Accuracy (smallest diff): 88.9%\n",
      "\n",
      "K=9\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 9\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 9\n",
      "Accuracy (smallest diff): 86.1%\n",
      "\n",
      "K=10\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 10\n",
      "Accuracy (largest diff): 97.2%\n",
      "Number of features in the selected dataset (smallest diff): 10\n",
      "Accuracy (smallest diff): 94.4%\n",
      "\n",
      "K=11\n",
      "Number of features in the dataset: 13\n",
      "Number of features in the selected dataset (largest diff): 11\n",
      "Accuracy (largest diff): 94.4%\n",
      "Number of features in the selected dataset (smallest diff): 11\n",
      "Accuracy (smallest diff): 94.4%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the dataset from scikit-learn\n",
    "wine = load_wine()\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "data = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# Split the input features and target variable\n",
    "X = data.iloc[:, :]\n",
    "y = wine.target\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns.tolist()\n",
    "number_of_features = X.shape[1]\n",
    "print(\"number_of_features: \",number_of_features)\n",
    "\n",
    "for k in range(5, 12):\n",
    "    print(f\"\\nK={k}\")\n",
    "    \n",
    "    # Perform K-means clustering with K clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Compute the mean value of each feature for each cluster\n",
    "    cluster_means = [np.mean(X[kmeans.labels_ == i], axis=0) for i in range(kmeans.n_clusters)]\n",
    "\n",
    "    # Compute the absolute difference in means between clusters for each feature\n",
    "    diff_means = np.abs(np.diff(cluster_means, axis=0))\n",
    "\n",
    "    # Select the features with the largest difference in means between clusters\n",
    "    selected_features = np.argsort(np.sum(diff_means, axis=0))[::-1][:k]\n",
    "    \n",
    "    # Select the features with the smallest difference in means between clusters\n",
    "    selected_features_smallest = np.argsort(np.sum(diff_means, axis=0))[:k]\n",
    "\n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    print(f\"Number of features in the dataset: {number_of_features}\")\n",
    "    print(f\"Number of features in the selected dataset (largest diff): {len(selected_features)}\")\n",
    "    print(f\"Accuracy (largest diff): {round(accuracy,1)}%\")\n",
    "    \n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected_smallest = X.iloc[:, selected_features_smallest]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_smallest, X_test_smallest, y_train_smallest, y_test_smallest = train_test_split(X_selected_smallest, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf_smallest = DecisionTreeClassifier(random_state=42)\n",
    "    clf_smallest.fit(X_train_smallest, y_train_smallest)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred_smallest = clf_smallest.predict(X_test_smallest)\n",
    "    accuracy_smallest = accuracy_score(y_test_smallest, y_pred_smallest)*100\n",
    "\n",
    "    print(f\"Number of features in the selected dataset (smallest diff): {len(selected_features_smallest)}\")\n",
    "    print(f\"Accuracy (smallest diff): {round(accuracy_smallest,1)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features in the dataset:  13\n",
      "k =  5 :\n",
      "Top 5 features with highest variance Accuracy: 86.1\n",
      "Top 5 features with lowest variance Accuracy: 83.3\n",
      "k =  6 :\n",
      "Top 6 features with highest variance Accuracy: 94.4\n",
      "Top 6 features with lowest variance Accuracy: 91.7\n",
      "k =  7 :\n",
      "Top 7 features with highest variance Accuracy: 94.4\n",
      "Top 7 features with lowest variance Accuracy: 88.9\n",
      "k =  8 :\n",
      "Top 8 features with highest variance Accuracy: 97.2\n",
      "Top 8 features with lowest variance Accuracy: 86.1\n",
      "k =  9 :\n",
      "Top 9 features with highest variance Accuracy: 94.4\n",
      "Top 9 features with lowest variance Accuracy: 86.1\n",
      "k =  10 :\n",
      "Top 10 features with highest variance Accuracy: 97.2\n",
      "Top 10 features with lowest variance Accuracy: 94.4\n",
      "k =  11 :\n",
      "Top 11 features with highest variance Accuracy: 94.4\n",
      "Top 11 features with lowest variance Accuracy: 94.4\n",
      "k =  12 :\n",
      "Top 12 features with highest variance Accuracy: 94.4\n",
      "Top 12 features with lowest variance Accuracy: 94.4\n",
      "k =  13 :\n",
      "Top 13 features with highest variance Accuracy: 94.4\n",
      "Top 13 features with lowest variance Accuracy: 94.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the wine dataset from scikit-learn\n",
    "wine = load_wine()\n",
    "\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "number_of_features = X.shape[1]\n",
    "print(\"number of features in the dataset: \",number_of_features)\n",
    "\n",
    "# Fit K-means clustering to the data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Compute the variance of each feature within each cluster\n",
    "variances = np.zeros((3, X.shape[1]))\n",
    "for i in range(3):\n",
    "    cluster_data = X[labels == i]\n",
    "    variances[i] = np.var(cluster_data, axis=0)\n",
    "\n",
    "# Select the features with the highest variance within the cluster with the most data points\n",
    "most_common_cluster = np.argmax(np.bincount(labels))\n",
    "\n",
    "for k in range(5, number_of_features+1):\n",
    "    # select the top k features with highest variance\n",
    "    selected_features = np.argsort(variances[most_common_cluster])[::-1][:k]\n",
    "    # select the top k features with lowest variance\n",
    "    low_selected_features = np.argsort(variances[most_common_cluster])[:k]\n",
    "\n",
    "    # Use the selected features to transform the data\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    low_X_selected = X.iloc[:, low_selected_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy for top k features with highest variance\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(\"k = \",k,\":\")\n",
    "    print(f\"Top {k} features with highest variance Accuracy:\", round(accuracy,1))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(low_X_selected, y, test_size=0.2, random_state=42)\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions on the test set and compute accuracy for top k features with lowest variance\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    print(f\"Top {k} features with lowest variance Accuracy:\", round(accuracy,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data - diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#loading dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\linor\\\\Desktop\\\\project\\\\diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "0                       0.627   50  \n",
       "1                       0.351   31  \n",
       "2                       0.672   32  \n",
       "3                       0.167   21  \n",
       "4                       2.288   33  \n",
       "..                        ...  ...  \n",
       "763                     0.171   63  \n",
       "764                     0.340   27  \n",
       "765                     0.245   30  \n",
       "766                     0.349   47  \n",
       "767                     0.315   23  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: Outcome, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chisquared test - diabetes_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "chi_scores = chi2(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 111.51969064, 1411.88704064,   17.60537322,   53.10803984,\n",
       "        2175.56527292,  127.66934333,    5.39268155,  181.30368904]),\n",
       " array([4.55261043e-026, 5.48728628e-309, 2.71819252e-005, 3.15697650e-013,\n",
       "        0.00000000e+000, 1.32590849e-029, 2.02213728e-002, 2.51638830e-041]))"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_values.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since DiabetesPedigreeFunction has higher the p-value,\n",
    "it says that this variables is independent of the repsone and can not be considered for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  Age  \\\n",
       "0            6      148             72             35        0  33.6   50   \n",
       "1            1       85             66             29        0  26.6   31   \n",
       "2            8      183             64              0        0  23.3   32   \n",
       "3            1       89             66             23       94  28.1   21   \n",
       "4            0      137             40             35      168  43.1   33   \n",
       "\n",
       "   Outcome  \n",
       "0        1  \n",
       "1        0  \n",
       "2        1  \n",
       "3        0  \n",
       "4        1  "
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop_1 = df.drop('DiabetesPedigreeFunction', axis=1)\n",
    "df_drop_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.8\n",
      "precision: 0.7\n",
      "Recall:  0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = df_drop_1.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df_drop_1.iloc[:, -1]   # Select the last column\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "model = model.fit(x_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "# Calculate precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate recall using sklearn's recall_score function\n",
    "recall =metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate F1 score using sklearn's f1_score function\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate mean squared error using sklearn's mean_squared_error function\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Accuracy:\",round(accuracy,1))\n",
    "print(\"precision:\",round(precision,1))\n",
    "print(\"Recall: \",round(recall,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the dataset: 8\n",
      "Number of features in the selected dataset: 5\n",
      "Accuracy: 63.6%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\linor\\\\Desktop\\\\project\\\\diabetes.csv\")\n",
    "\n",
    "# Split the input features and target variable\n",
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns.tolist()\n",
    "number_of_features = X.shape[1]\n",
    "\n",
    "# Perform K-means clustering with K=3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Compute the mean value of each feature for each cluster\n",
    "cluster_means = [np.mean(X[kmeans.labels_ == i], axis=0) for i in range(kmeans.n_clusters)]\n",
    "\n",
    "# Compute the absolute difference in means between clusters for each feature\n",
    "diff_means = np.abs(np.diff(cluster_means, axis=0))\n",
    "\n",
    "# Select the features with the largest difference in means between clusters\n",
    "selected_features = np.argsort(np.sum(diff_means, axis=0))[::-1][:5]\n",
    "\n",
    "# Filter the input data to keep only the selected features\n",
    "X_selected = X.iloc[:, selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier on the training set\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compute accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "print(f\"Number of features in the dataset: {number_of_features}\")\n",
    "print(f\"Number of features in the selected dataset: {len(selected_features)}\")\n",
    "print(f\"Accuracy: {round(accuracy,1)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K=5\n",
      "Number of features in the dataset: 8\n",
      "Number of features in the selected dataset: 5\n",
      "Accuracy: 64.3%\n",
      "Number of features in the selected dataset (smallest diff): 5\n",
      "Accuracy (smallest diff): 58.4%\n",
      "\n",
      "K=6\n",
      "Number of features in the dataset: 8\n",
      "Number of features in the selected dataset: 6\n",
      "Accuracy: 66.9%\n",
      "Number of features in the selected dataset (smallest diff): 6\n",
      "Accuracy (smallest diff): 55.8%\n",
      "\n",
      "K=7\n",
      "Number of features in the dataset: 8\n",
      "Number of features in the selected dataset: 7\n",
      "Accuracy: 67.5%\n",
      "Number of features in the selected dataset (smallest diff): 7\n",
      "Accuracy (smallest diff): 72.1%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\linor\\\\Desktop\\\\project\\\\diabetes.csv\")\n",
    "\n",
    "# Split the input features and target variable\n",
    "X = data.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = data.iloc[:, -1]   # Select the last column\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = X.columns.tolist()\n",
    "number_of_features = X.shape[1]\n",
    "\n",
    "for k in range(5, 8):\n",
    "    print(f\"\\nK={k}\")\n",
    "    \n",
    "    # Perform K-means clustering with K clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Compute the mean value of each feature for each cluster\n",
    "    cluster_means = [np.mean(X[kmeans.labels_ == i], axis=0) for i in range(kmeans.n_clusters)]\n",
    "\n",
    "    # Compute the absolute difference in means between clusters for each feature\n",
    "    diff_means = np.abs(np.diff(cluster_means, axis=0))\n",
    "\n",
    "    # Select the features with the largest difference in means between clusters\n",
    "    selected_features = np.argsort(np.sum(diff_means, axis=0))[::-1][:k]\n",
    "    \n",
    "    # Select the features with the smallest difference in means between clusters\n",
    "    selected_features_smallest = np.argsort(np.sum(diff_means, axis=0))[::-1][-k:]\n",
    "\n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    print(f\"Number of features in the dataset: {number_of_features}\")\n",
    "    print(f\"Number of features in the selected dataset: {len(selected_features)}\")\n",
    "    print(f\"Accuracy: {round(accuracy,1)}%\")\n",
    "    \n",
    "    # Filter the input data to keep only the selected features\n",
    "    X_selected_smallest = X.iloc[:, selected_features_smallest]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_smallest, X_test_smallest, y_train_smallest, y_test_smallest = train_test_split(X_selected_smallest, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf_smallest = DecisionTreeClassifier(random_state=42)\n",
    "    clf_smallest.fit(X_train_smallest, y_train_smallest)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred_smallest = clf_smallest.predict(X_test_smallest)\n",
    "    accuracy_smallest = accuracy_score(y_test_smallest, y_pred_smallest)*100\n",
    "\n",
    "    print(f\"Number of features in the selected dataset (smallest diff): {len(selected_features_smallest)}\")\n",
    "    print(f\"Accuracy (smallest diff): {round(accuracy_smallest,1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection by clustering - diabetes_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features in the dataset:  8\n",
      "k =  4 :\n",
      "Accuracy with top 4 features with highest variance: 63.6\n",
      "Accuracy with top 4 features with lowest variance: 58.4\n",
      "k =  5 :\n",
      "Accuracy with top 5 features with highest variance: 64.9\n",
      "Accuracy with top 5 features with lowest variance: 58.4\n",
      "k =  6 :\n",
      "Accuracy with top 6 features with highest variance: 67.5\n",
      "Accuracy with top 6 features with lowest variance: 57.8\n",
      "k =  7 :\n",
      "Accuracy with top 7 features with highest variance: 67.5\n",
      "Accuracy with top 7 features with lowest variance: 64.3\n",
      "k =  8 :\n",
      "Accuracy with top 8 features with highest variance: 75.3\n",
      "Accuracy with top 8 features with lowest variance: 76.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data from a CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\linor\\\\Desktop\\\\project\\\\diabetes.csv\")\n",
    "\n",
    "X = df.iloc[:, :-1]  # Select all columns except the last one\n",
    "y = df.iloc[:, -1]   # Select the last column\n",
    "print(\"number of features in the dataset: \", X.shape[1])\n",
    "# Fit K-means clustering to the data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Compute the variance of each feature within each cluster\n",
    "variances = np.zeros((3, X.shape[1]))\n",
    "for i in range(3):\n",
    "    cluster_data = X[labels == i]\n",
    "    variances[i] = np.var(cluster_data, axis=0)\n",
    "    #print(cluster_data)\n",
    "\n",
    "# Select the features with the highest variance within the cluster with the most data points\n",
    "most_common_cluster = np.argmax(np.bincount(labels))\n",
    "selected_features = np.argsort(variances[most_common_cluster])[::-1] # select features with highest variance\n",
    "low_selected_features = np.argsort(variances[most_common_cluster])  # select features with lowest variance\n",
    "\n",
    "# Loop over k values and print the accuracy for each\n",
    "for k in range(4, 9):\n",
    "    # Select the top k features with highest variance\n",
    "    selected_k_features = selected_features[:k]\n",
    "    X_selected_k = X.iloc[:, selected_k_features]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected_k, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_high_var = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    # Select the top k features with lowest variance\n",
    "    low_selected_k_features = low_selected_features[:k]\n",
    "    X_low_selected_k = X.iloc[:, low_selected_k_features]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_low_selected_k, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a decision tree classifier on the training set\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_low_var = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    # Print the accuracy for the current k value\n",
    "    print(\"k = \",k,\":\")\n",
    "    print(f\"Accuracy with top {k} features with highest variance: {round(accuracy_high_var, 1)}\")\n",
    "    print(f\"Accuracy with top {k} features with lowest variance: {round(accuracy_low_var, 1)}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
